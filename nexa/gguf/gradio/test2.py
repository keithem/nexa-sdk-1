import gradio as gr
from nexa.gguf.nexa_inference_text import NexaTextInference
from nexa.gguf.llama._utils_transformers import suppress_stdout_stderr
from nexa.general import pull_model

# -----------------------------
# 1) åˆå§‹åŒ–æ¨¡å‹ç›¸å…³å˜é‡ï¼ˆä¸éœ€æ±‚ä¿æŒä¸€è‡´ï¼šé»˜è®¤æœ¬åœ°è·¯å¾„ï¼Œæ— æ¨¡å‹é€‰æ‹©ï¼‰
# -----------------------------
nexa_model = None
is_local_path = True
hf = False
default_model = "Llama3.2-3B-Instruct:q4_0"  # ç¬¬2æ¡ï¼šä¿®æ”¹ä¸ºæ­¤é»˜è®¤å€¼

# è¿™é‡ŒæŒ‡å®šæ¨¡å‹åœ¨æœ¬åœ°çš„å®é™…æ–‡ä»¶è·¯å¾„
local_model_path = "/Users/chenzekai/.cache/nexa/hub/official/Llama3.2-3B-Instruct/q4_0.gguf"

# -----------------------------
# åŠ è½½æ¨¡å‹å‡½æ•°
# -----------------------------
def load_model(model_path):
    """
    åŠ è½½æ–‡æœ¬æ¨¡å‹ã€‚ç”±äº is_local_path=True, æˆ‘ä»¬å°†ç›´æ¥åŠ è½½æœ¬åœ°è·¯å¾„ã€‚
    """
    nexa_model = NexaTextInference(
        model_path=model_path,
        local_path=local_model_path,
        # å¯æŒ‰éœ€æ·»åŠ æ›´å¤šåˆå§‹åŒ–å‚æ•°ï¼Œå¦‚: temperature, max_new_tokens ç­‰
    )
    return nexa_model

# å°è¯•åŠ è½½æ¨¡å‹
try:
    nexa_model = load_model(default_model)
except Exception as e:
    nexa_model = None
    print(f"Failed to load model: {e}")

# -----------------------------
# æ¨ç†å‡½æ•°ï¼šæ ¹æ®ç”¨æˆ·è¾“å…¥ï¼ˆæ–‡æœ¬ï¼‰å’Œè¶…å‚æ•°ï¼Œè¿”å›ç”Ÿæˆç»“æœ
# -----------------------------
def process_text_fn(user_input, temperature, max_new_tokens, top_k, top_p, nctx):
    """
    å°†æ–‡æœ¬è¾“å…¥å’Œæ¨¡å‹å‚æ•°ä¼ ç»™ NexaTextInference è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼Œå¹¶è¿”å›ç»“æœã€‚
    """
    if not nexa_model:
        return "No model loaded. Please check the local path or model loading."

    # å…ˆæ›´æ–°æ¨¡å‹å†…éƒ¨ä½¿ç”¨çš„ç”Ÿæˆå‚æ•°
    nexa_model.params.update({
        "temperature": temperature,
        "max_new_tokens": max_new_tokens,
        "top_k": top_k,
        "top_p": top_p,
        "nctx": nctx,
    })

    try:
        # å¦‚æœæ¨¡å‹æ˜¯ chat_format çš„ï¼Œåˆ™ç”¨ _chatï¼Œå¦åˆ™ç”¨ _complete
        if hasattr(nexa_model, "chat_format") and nexa_model.chat_format:
            # æµå¼è¾“å‡º
            result = ""
            with suppress_stdout_stderr():
                for chunk in nexa_model._chat(user_input):
                    choice = chunk["choices"][0]
                    if "delta" in choice:
                        delta = choice["delta"]
                        content = delta.get("content", "")
                    else:
                        content = choice.get("text", "")
                    result += content
            return result
        else:
            # æ™®é€šæ–‡æœ¬ç”Ÿæˆ
            result = ""
            with suppress_stdout_stderr():
                for chunk in nexa_model._complete(user_input):
                    choice = chunk["choices"][0]
                    if "text" in choice:
                        delta = choice["text"]
                    elif "delta" in choice:
                        delta = choice["delta"]["content"]
                    else:
                        delta = ""
                    result += delta
            return result

    except Exception as e:
        return f"Error during text generation: {e}"

# -----------------------------
# æ„å»º Gradio ç•Œé¢
# -----------------------------
with gr.Blocks() as demo:
    # ç¬¬3æ¡ï¼šä¿®æ”¹æ ‡é¢˜ä¸ºâ€œNexa AI Text Generationâ€
    gr.HTML(
        """
        <div style="display: flex; align-items: center; margin-bottom: 5px; padding-top: 10px;">
            <h1 style="font-family: Arial, sans-serif; font-size: 2.5em; font-weight: bold; margin: 0; padding-bottom: 5px;">
                Nexa AI Text Generation
            </h1>
            <a href='https://github.com/NexaAI/nexa-sdk' style='text-decoration: none; margin-left: 10px;'>
                <img src='https://img.shields.io/badge/SDK-Nexa-blue' alt='Nexa SDK' style='vertical-align: middle;'>
            </a>
        </div>
        """
    )

    gr.HTML(
        f"""
        <div style="font-family: Arial, sans-serif; font-size: 1em; color: #444;">
            <b>Powered by Nexa AI SDKğŸ™</b> <br>
            <b>Model path: {default_model}</b>
        </div>
        """
    )

    # ç¬¬5æ¡ï¼šGeneration Parameters æ¿å—
    with gr.Group():
        gr.HTML(
            """
            <h2 style='font-family: Arial, sans-serif; font-size: 1.3em; font-weight: bold; margin-bottom: 0.2em;'>
                Generation Parameters
            </h2>
            """
        )
        with gr.Row():
            temperature_slider = gr.Slider(
                label="Temperature",
                minimum=0.00,
                maximum=1.00,
                step=0.01,
                value=0.7  # å¯æ ¹æ®éœ€è¦é»˜è®¤è®¾ä¸€ä¸ª
            )
            max_tokens_slider = gr.Slider(
                label="Max New Tokens",
                minimum=1,
                maximum=500,
                step=1,
                value=256  # å¯æ ¹æ®éœ€è¦é»˜è®¤è®¾ä¸€ä¸ª
            )
        with gr.Row():
            top_k_slider = gr.Slider(
                label="Top K",
                minimum=1,
                maximum=100,
                step=1,
                value=50
            )
            top_p_slider = gr.Slider(
                label="Top P",
                minimum=0.00,
                maximum=1.00,
                step=0.01,
                value=1.0
            )
        with gr.Row():
            nctx_slider = gr.Slider(
                label="Context length",
                minimum=1000,
                maximum=9999,
                step=1,
                value=2048
            )

    # ç¬¬4æ¡ï¼šåªä¿ç•™ä¸€ä¸ªæ–‡æœ¬è¾“å…¥æ¡†, placeholder="Say something"
    user_text_input = gr.Textbox(
        placeholder="Say something",
        lines=3,
        label="",  # éœ€æ±‚é‡Œæåˆ°ï¼šå»æ‰ label
    )

    # ç¬¬6æ¡ï¼šç‚¹å‡» Send åè¾“å‡ºåˆ° â€œModel Responseâ€
    send_button = gr.Button("Send")
    model_response = gr.Textbox(label="Model Response", interactive=False)

    # Send æŒ‰é’®ç‚¹å‡»åæ‰§è¡Œ
    send_button.click(
        fn=process_text_fn,
        inputs=[
            user_text_input,
            temperature_slider,
            max_tokens_slider,
            top_k_slider,
            top_p_slider,
            nctx_slider
        ],
        outputs=model_response
    )

# -----------------------------
# å¯åŠ¨ Gradio æœåŠ¡
# -----------------------------
if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860, share=True)
