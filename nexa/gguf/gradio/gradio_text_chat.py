import gradio as gr
from nexa.gguf.nexa_inference_text import NexaTextInference
from nexa.general import pull_model

# --------------------------------------------------------------------
# 1. åˆå§‹åŒ–æ¨¡å‹ç›¸å…³çš„å˜é‡ï¼ˆä¿ç•™åŸå…ˆçš„ is_local_path, hf, nexa_model ç­‰ï¼‰
# --------------------------------------------------------------------
nexa_model = None
is_local_path = True
hf = False

# --------------------------------------------------------------------
# 2. é»˜è®¤æ¨¡å‹åŠå…¶æœ¬åœ°è·¯å¾„ï¼ˆä¸åšæ¨¡å‹é€‰æ‹©ï¼Œä¿æŒæœ¬åœ°åŠ è½½ï¼‰
# --------------------------------------------------------------------
default_model = "Llama3.2-3B-Instruct:q4_0"
local_model_path = "/Users/chenzekai/.cache/nexa/hub/official/Llama3.2-3B-Instruct/q4_0.gguf"

# --------------------------------------------------------------------
# 3. åŠ è½½æ¨¡å‹å‡½æ•°
# --------------------------------------------------------------------
def load_model(model_path):
    """
    å› ä¸º is_local_path=Trueï¼Œè¿™é‡Œç›´æ¥åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹ã€‚
    å¦‚æœéœ€è¦æ›´å¤šåˆå§‹åŒ–å‚æ•°ï¼Œå¯åœ¨æ­¤å¤„æ·»åŠ ã€‚
    """
    try:
        # å¦‚æœæ¨¡å‹å·²ç»åœ¨æœ¬åœ°ï¼Œä¸éœ€è¦å†æ¬¡ä»è¿œç¨‹ pullï¼Œåˆ™å¯æ³¨é‡Š pull_model
        # local_path, _ = pull_model(model_path)
        nexa_model_instance = NexaTextInference(
            model_path=model_path,
            local_path=local_model_path
        )
        return nexa_model_instance
    except Exception as e:
        print(f"Failed to load model: {e}")
        return None

# --------------------------------------------------------------------
# 4. åœ¨å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹
# --------------------------------------------------------------------
try:
    nexa_model = load_model(default_model)
except Exception as e:
    nexa_model = None
    print(f"Failed to load model: {e}")

# --------------------------------------------------------------------
# 5. å¤„ç†æ–‡æœ¬è¾“å…¥çš„å‡½æ•°ï¼ˆæ›´æ–°ï¼šæ¶ˆè´¹ç”Ÿæˆå™¨å¹¶è¿”å›å®Œæ•´æ–‡æœ¬ï¼‰
# --------------------------------------------------------------------
def process_text_fn(
    user_input,
    temperature,
    max_new_tokens,
    top_k,
    top_p,
    context_length
):
    """
    å°†ç”¨æˆ·æ–‡æœ¬ä¸ç”Ÿæˆå‚æ•°ä¸€èµ·ä¼ é€’ç»™æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œå¹¶è¿”å›æ¨¡å‹è¾“å‡ºã€‚
    """
    if not nexa_model:
        return "No model loaded. Please load a model first."
    if not user_input:
        return "Please provide a prompt."

    try:
        # æ›´æ–°æ¨¡å‹å†…éƒ¨å‚æ•°
        nexa_model.params["temperature"] = temperature
        nexa_model.params["max_new_tokens"] = max_new_tokens
        nexa_model.params["top_k"] = top_k
        nexa_model.params["top_p"] = top_p
        nexa_model.params["nctx"] = context_length

        # åŸæœ¬ _complete() è¿”å›ç”Ÿæˆå™¨ï¼Œè¿™é‡Œé€ä¸ªæ‹¼æ¥ç”Ÿæˆå®Œæ•´å­—ç¬¦ä¸²
        response_iter = nexa_model._complete(user_input)
        response_text = ""
        for chunk in response_iter:
            choice = chunk["choices"][0]
            if "text" in choice:
                token = choice["text"]
            elif "delta" in choice and "content" in choice["delta"]:
                token = choice["delta"]["content"]
            else:
                token = ""
            response_text += token

        return response_text

    except Exception as e:
        return f"Error during text generation: {e}"

# --------------------------------------------------------------------
# 6. Gradio ç•Œé¢æ­å»º
# --------------------------------------------------------------------
with gr.Blocks() as demo:
    # æ ‡é¢˜å’Œå¾½æ ‡
    gr.HTML(
        """
        <div style="display: flex; align-items: center; margin-bottom: 5px; padding-top: 10px;">
            <h1 style="font-family: Arial, sans-serif; font-size: 2.5em; font-weight: bold; margin: 0; padding-bottom: 5px;">
                Nexa AI Text Generation
            </h1>
            <a href='https://github.com/NexaAI/nexa-sdk' style='text-decoration: none; margin-left: 10px;'>
                <img src='https://img.shields.io/badge/SDK-Nexa-blue' alt='Nexa SDK' style='vertical-align: middle;'>
            </a>
        </div>
        """
    )
    # Powered by + Model path
    gr.HTML(
        f"""
        <div style="font-family: Arial, sans-serif; font-size: 1em; color: #444;">
            <b>Powered by Nexa AI SDKğŸ™</b> <br>
            <b>Model path: {default_model}</b>
        </div>
        """
    )

    # -----------
    # Generation Parameters
    # -----------
    gr.HTML("<h3 style='font-family: Arial, sans-serif; font-size: 1.2em; font-weight: bold;'>Generation Parameters</h3>")
    with gr.Row():
        temperature_slider = gr.Slider(
            label="Temperature",
            minimum=0.0, maximum=1.0, step=0.01, value=0.7
        )
        max_new_tokens_slider = gr.Slider(
            label="Max New Tokens",
            minimum=1, maximum=500, step=1, value=128
        )
    with gr.Row():
        top_k_slider = gr.Slider(
            label="Top K",
            minimum=1, maximum=100, step=1, value=40
        )
        top_p_slider = gr.Slider(
            label="Top P",
            minimum=0.0, maximum=1.0, step=0.01, value=0.9
        )
    with gr.Row():
        context_length_slider = gr.Slider(
            label="Context length",
            minimum=1000, maximum=9999, step=1, value=2048
        )

    # ----------
    # æ–‡æœ¬è¾“å…¥
    # ----------
    user_input_box = gr.Textbox(
        placeholder="Say something...",
        lines=3,
        show_label=False
    )

    # -----------
    # è§¦å‘æŒ‰é’® + è¾“å‡º
    # -----------
    send_button = gr.Button("Send")
    model_response = gr.Textbox(
        label="Model Response",
        interactive=False
    )

    send_button.click(
        fn=process_text_fn,
        inputs=[
            user_input_box,
            temperature_slider,
            max_new_tokens_slider,
            top_k_slider,
            top_p_slider,
            context_length_slider
        ],
        outputs=model_response
    )

# --------------------------------------------------------------------
# 7. å¯åŠ¨ Gradio åº”ç”¨
# --------------------------------------------------------------------
if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860, share=True)
